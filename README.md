# TalkingSlides

![Alt text](frontend/public/logo.PNG)

## Inspiration  
In an era where access to quality education is more crucial than ever, we recognized a gap in how students review lecture materials when they miss class. Whether due to unforeseen circumstances, emergencies, or scheduling conflicts, students often struggle to catch up on missed lectures effectively. Our team was inspired to create a tool that transforms static lecture slides into engaging video lectures, ensuring that no student is left behind in their learning journey.  

## What it does  
Our application allows students to upload lecture slides and supplement them with synthesized speech and AI-generated video, creating a dynamic video lecture experience. Users can input an image of a professor or a preferred avatar, and our system will generate a realistic video presentation that aligns with the lecture content. This tool enhances comprehension by mimicking real lecture delivery, including voice modulation and visual engagement.  

## How we built it  
We developed the backend using Django to manage data processing and AI-driven video synthesis. The frontend is built with React, ensuring a seamless and user-friendly experience. For speech generation, we integrate advanced text-to-speech models, while deep learning techniques power the avatar animation. Additionally, we incorporate OCR (Optical Character Recognition) to extract text from lecture slides and synchronize it with the generated video content.  

## Challenges we ran into  
One of the main challenges was ensuring natural-looking video synthesis that aligns well with speech and lecture content. Another hurdle was processing and structuring lecture slides to create coherent video sequences without requiring extensive manual input from users. Optimizing AI models for efficiency without sacrificing quality was also a key technical challenge.  

## Accomplishments that we're proud of  
We are proud to have developed a tool that democratizes access to lecture content, making learning more accessible and engaging. Our application successfully bridges the gap between static notes and immersive lectures, giving students an innovative way to review material even when they miss class.  

## What we learned  
Through this project, we deepened our understanding of AI-driven media synthesis, real-time text processing, and interactive front-end development. We also learned how to design a system that balances automation with user customization, ensuring a flexible yet intuitive experience.  

## What's next  
Moving forward, we plan to refine our avatar generation models to support real-time lip-syncing for even more realistic presentations. Additionally, we aim to expand customization options, allowing users to select different speaking styles and voice tones. Our long-term vision is to integrate multilingual support, making our tool beneficial for students across diverse educational backgrounds.  
